function [traj, infStates] = tapas_hgf_ar1_reward_social(r, p, varargin)
% Calculates the trajectories of the agent's representations under the HGF for AR(1) processes
% and binary outcomes.
%
% This function can be called in two ways:
% 
% (1) tapas_hgf_ar1_binary(r, p)
%   
%     where r is the structure generated by tapas_fitModel and p is the parameter vector in native space;x%
% (2) tapas_hgf_ar1_binary(r, ptrans, 'trans')
% 
%     where r is the structure generated by tapas_fitModel, ptrans is the parameter vector in
%     transformed space, and 'trans' is a flag indicating this.
%
% --------------------------------------------------------------------------------------------------
% Copyright (C) 2012-2017 Christoph Mathys, TNU, UZH & ETHZ
%
% This file is part of the HGF toolbox, which is released under the terms of the GNU General Public
% Licence (GPL), version 3. You can redistribute it and/or modify it under the terms of the GPL
% (either version 3 or, at your option, any later version). For further details, see the file
% COPYING or <http://www.gnu.org/licenses/>.
%  test to see whats in p
% x = r.c_prc;
% s = r.c_obs;
% p = p;

% Transform paramaters back to their native space if needed
if ~isempty(varargin) && strcmp(varargin{1},'trans');
    p = tapas_hgf_ar1_reward_social_transp(r, p);
end

% Number of levels
try
    l = r.c_prc.n_levels;
catch
    l = (length(p)+1)/5;
    
    if l ~= floor(l)
        error('tapas:hgf:UndetNumLevels', 'Cannot determine number of levels');
    end
end


% Unpack parameters
% mu_r_0 = p(1:l) %% Hier index f�r a und r
% mu_a_0 = p(1:l)
% sa_r_0 = p(l+1:2*l);
% sa_a_0 = p(l+1:2*l);
% phi_r  = p(2*l+1:3*l);
% phi_a  = p(2*l+1:3*l);
% m_r    = p(3*l+1:4*l);
% m_a    = p(3*l+1:4*l);
% ka_r   = p(4*l+1:5*l-1);
% ka_a   = p(4*l+1:5*l-1);
% om_r   = p(5*l:6*l-2);
% om_a   = p(5*l:6*l-2);
% th_r  = exp(p(6*l-1));
% th_a   = exp(p(6*l-1));

% Unpack parameters
% 
% mu_r_0 = p(1:l); %% Hier index f�r a und r
% sa_r_0 = p(l+1:2*l);
% phi_r  = p(2*l+1:3*l);
% m_r    = p(3*l+1:4*l);
% ka_r   = p(4*l+1:5*l-1);
% om_r   = p(5*l:6*l-2);
% th_r  = exp(p(6*l-1));
% 
% mu_a_0 = p(6*l:7*l-1) ;%% Hier index f�r a und r
% sa_a_0 = p(7*l:8*l-1);
% phi_a  = p(8*l:9*l-1);
% m_a    = p(9*l:10*l-2);
% ka_a   = p(10*l-1:10*l);
% om_a   = p(10*l+1:11*l-1);
% th_a  = exp(p(11*l));

% 
mu_r_0 = p(1:l);
sa_r_0 = p(l+1:2*l);
phi_r  = p(2*l+1:3*l);
m_r    = p(3*l+1:4*l);
ka_r   = p(4*l+1:5*l-1);
om_r   = p(5*l:6*l-2);
th_r  = exp(p(6*l-1));

mu_a_0 = p(6*l:7*l-1) ;
sa_a_0 = p(7*l:8*l-1);
phi_a  = p(8*l:9*l-1);
m_a    = p(9*l:10*l-1);
ka_a   = p(10*l:10*l+1);
om_a   = p(11*l-1:11*l);
th_a  = exp(p(11*l+1));



% Add dummy "zeroth" trial
u_r = [0; r.u(:,1)];
u_a = [0; r.u(:,2)];

% Number of trials (including prior)
n_r = length(u_r);
n_a = length(u_a);


% Assume that if u has more than one column, the last contains t

try
    if r.c_prc.irregular_intervals
        if size(u_r,2) > 1
            t_r = [0; r.u_r(:,end)];
        else
            error('tapas:hgf:InputSingleColumn', 'Input matrix must contain more than one column if irregular_intervals is set to true.');
        end
    else
        t_r = ones(n_r,1);
    end
catch
    if size(u_r,2) > 1
        t_r = [0; r.u_r(:,end)];
    else
        t_r = ones(n_r,1);
    end
end

try
    if r.c_prc.irregular_intervals
        if size(u_a,2) > 1
            t_a = [0; r.u_a(:,end)];
        else
            error('tapas:hgf:InputSingleColumn', 'Input matrix must contain more than one column if irregular_intervals is set to true.');
        end
    else
        t_a = ones(n_a,1);
    end
catch
    if size(u_a,2) > 1
        t_a = [0; r.u_a(:,end)];
    else
        t_a = ones(n_a,1);
    end
end



% Initialize updated quantities

% Representations
mu_r = NaN(n_r,l);
pi_r = NaN(n_r,l);
mu_a = NaN(n_a,l);
pi_a = NaN(n_a,l);

% Other quantities
muhat_r = NaN(n_r,l);
pihat_r = NaN(n_r,l);
v_r     = NaN(n_r,l);
w_r     = NaN(n_r,l-1);
da_r    = NaN(n_r,l);

muhat_a = NaN(n_a,l);
pihat_a = NaN(n_a,l);
v_a     = NaN(n_a,l);
w_a     = NaN(n_a,l-1);
da_a    = NaN(n_a,l);

% Representation priors
% Note: first entries of the other quantities remain
% NaN because they are undefined and are thrown away
% at the end; their presence simply leads to consistent
% trial indices.

mu_r(1,1) = tapas_sgm(mu_r_0(1), 1);% sigmoid von NaN? Geht doch nicht
pi_r(1,1) = Inf;
mu_r(1,2:end) = mu_r_0(2:end);
pi_r(1,2:end) = 1./sa_r_0(2:end);
mu_a(1,1) = tapas_sgm(mu_a_0(1), 1);
pi_a(1,1) = Inf;
mu_a(1,2:end) = mu_a_0(2:end);
pi_a(1,2:end) = 1./sa_a_0(2:end);

% Pass through representation update loop
for k = 2:1:n_r
    if not(ismember(k-1, r.ign))
        
        %%%%%%%%%%%%%%%%%%%%%%
        % Effect of input u(k)
        %%%%%%%%%%%%%%%%%%%%%%
        
        % 2nd level prediction
        muhat_r(k,2) = mu_r(k-1,2) +t_r(k) *phi_r(2) *(m_r(2) -mu_r(k-1,2));
        muhat_a(k,2) = mu_a(k-1,2) +t_a(k) *phi_a(2) *(m_a(2) -mu_a(k-1,2));
       
        
        
        % 1st level
        % ~~~~~~~~~
        % Prediction
        muhat_r(k,1) = tapas_sgm(ka_r(1) *muhat_r(k,2), 1);
        muhat_a(k,1) = tapas_sgm(ka_a(1) *muhat_a(k,2), 1);
        
        % Precision of prediction
        pihat_r(k,1) = 1/(muhat_r(k,1)*(1 -muhat_r(k,1)));
        pihat_a(k,1) = 1/(muhat_a(k,1)*(1 -muhat_a(k,1)));

        % Updates
        pi_r(k,1) = Inf;
        pi_a(k,1) = Inf;
        
        mu_r(k,1) = u_r(k);
        mu_a(k,1) = u_a(k);

        % Prediction error
        da_r(k,1) = mu_r(k,1) -muhat_r(k,1);
        da_a(k,1) = mu_a(k,1) -muhat_a(k,1);

        % 2nd level
        % ~~~~~~~~~
        % Prediction: see above
        
        % Precision of prediction
        pihat_r(k,2) = 1/(1/pi_r(k-1,2) +exp(ka_r(2) *mu_r(k-1,3) +om_r(2)));
        pihat_a(k,2) = 1/(1/pi_a(k-1,2) +exp(ka_a(2) *mu_a(k-1,3) +om_a(2)));

        % Updates
        pi_r(k,2) = pihat_r(k,2) +ka_r(1)^2/pihat_r(k,1);
        pi_a(k,2) = pihat_a(k,2) +ka_a(1)^2/pihat_a(k,1);
        
        mu_r(k,2) = muhat_r(k,2) +ka_r(1)/pi_r(k,2) *da_r(k,1);
        mu_a(k,2) = muhat_a(k,2) +ka_a(1)/pi_a(k,2) *da_a(k,1);

        % Volatility prediction error
        da_r(k,2) = (1/pi_r(k,2) +(mu_r(k,2) -muhat_r(k,2))^2) *pihat_r(k,2) -1;
        da_a(k,2) = (1/pi_a(k,2) +(mu_a(k,2) -muhat_a(k,2))^2) *pihat_a(k,2) -1;

        if l > 3
            % Pass through higher levels
            % ~~~~~~~~~~~~~~~~~~~~~~~~~~
            for j = 3:l-1
                % Prediction
                muhat_r(k,j) = mu_r(k-1,j) +t_r(k) *phi_r(j) *(m_r(j) -mu_r(k-1,j));
                muhat_a(k,j) = mu_a(k-1,j) +t_a(k) *phi_a(j) *(m_a(j) -mu_a(k-1,j));
                
                % Precision of prediction
                pihat_r(k,j) = 1/(1/pi_r(k-1,j) +t_r(k) *exp(ka_r(j) *mu_r(k-1,j+1) +om_r(j)));
                pihat_a(k,j) = 1/(1/pi_a(k-1,j) +t_a(k) *exp(ka_a(j) *mu_a(k-1,j+1) +om_a(j)));

                % Weighting factor
                v_r(k,j-1) = t_r(k) *exp(ka_r(j-1) *mu_r(k-1,j) +om_r(j-1));
                   v_a(k,j-1) = t_a(k) *exp(ka_a(j-1) *mu_a(k-1,j) +om_a(j-1));
                w_r(k,j-1) = v_r(k,j-1) *pihat_r(k,j-1);
                  w_a(k,j-1) = v_a(k,j-1) *pihat_a(k,j-1);

                % Updates
                pi_r(k,j) = pihat_r(k,j) +1/2 *ka_r(j-1)^2 *w_r(k,j-1) *(w_r(k,j-1) +(2 *w_r(k,j-1) -1) *da_r(k,j-1));
                pi_a(k,j) = pihat_a(k,j) +1/2 *ka_a(j-1)^2 *w_a(k,j-1) *(w_a(k,j-1) +(2 *w_a(k,j-1) -1) *da_a(k,j-1));

                if pi_r(k,j) <= 0
                    error('tapas:hgf:NegPostPrec', 'Negative posterior precision in reward learning. Parameters are in a region where model assumptions are violated.');
                end
                if pi_a(k,j) <= 0
                    error('tapas:hgf:NegPostPrec', 'Negative posterior precision in social learning. Parameters are in a region where model assumptions are violated.');
                end

                mu_r(k,j) = muhat_r(k,j) +1/2 *1/pi_r(k,j) *ka_r(j-1) *w_r(k,j-1) *da_r(k,j-1);
                mu_a(k,j) = muhat_a(k,j) +1/2 *1/pi_a(k,j) *ka_a(j-1) *w_a(k,j-1) *da_a(k,j-1);
                % Volatility prediction error
                da_r(k,j) = (1/pi_r(k,j) +(mu_r(k,j) -muhat_r(k,j))^2) *pihat_r(k,j) -1;
                da_a(k,j) = (1/pi_a(k,j) +(mu_a(k,j) -muhat_a(k,j))^2) *pihat_a(k,j) -1;
            end
        end

        % Last level
        % ~~~~~~~~~~
        % Prediction
        muhat_r(k,l) = mu_r(k-1,l) +t_r(k) *phi_r(l) *(m_r(l) -mu_r(k-1,l));
        muhat_a(k,l) = mu_a(k-1,l) +t_a(k) *phi_a(l) *(m_a(l) -mu_a(k-1,l));
        
        % Precision of prediction
        pihat_r(k,l) = 1/(1/pi_r(k-1,l) +t_r(k) *th_r);
        pihat_a(k,l) = 1/(1/pi_a(k-1,l) +t_a(k) *th_a);

        % Weighting factor
        v_r(k,l)   = t_r(k) *th_r;
        v_r(k,l-1) = t_r(k) *exp(ka_r(l-1) *mu_r(k-1,l) +om_r(l-1));
        v_a(k,l)   = t_a(k) *th_a;
        v_a(k,l-1) = t_a(k) *exp(ka_a(l-1) *mu_a(k-1,l) +om_a(l-1));
        
        w_r(k,l-1) = v_r(k,l-1) *pihat_r(k,l-1);
        w_a(k,l-1) = v_a(k,l-1) *pihat_a(k,l-1);
        
        % Updates
        pi_r(k,l) = pihat_r(k,l) +1/2 *ka_r(l-1)^2 *w_r(k,l-1) *(w_r(k,l-1) +(2 *w_r(k,l-1) -1) *da_r(k,l-1));
        pi_a(k,l) = pihat_a(k,l) +1/2 *ka_a(l-1)^2 *w_a(k,l-1) *(w_a(k,l-1) +(2 *w_a(k,l-1) -1) *da_a(k,l-1));
        
        if pi_r(k,l) <= 0
            error('tapas:hgf:NegPostPrec', 'Negative posterior precision in reward learning. Parameters are in a region where model assumptions are violated.');
        end
        
          if pi_a(k,l) <= 0
            error('tapas:hgf:NegPostPrec', 'Negative posterior precision in social learning. Parameters are in a region where model assumptions are violated.');
        end

        mu_r(k,l) = muhat_r(k,l) +1/2 *1/pi_r(k,l) *ka_r(l-1) *w_r(k,l-1) *da_r(k,l-1);
        mu_a(k,l) = muhat_a(k,l) +1/2 *1/pi_a(k,l) *ka_a(l-1) *w_a(k,l-1) *da_a(k,l-1);
    
        % Volatility prediction error
        da_r(k,l) = (1/pi_r(k,l) +(mu_r(k,l) -muhat_r(k,l))^2) *pihat_r(k,l) -1;
        da_a(k,l) = (1/pi_a(k,l) +(mu_a(k,l) -muhat_a(k,l))^2) *pihat_a(k,l) -1;
    else

        mu_r(k,:) = mu_r(k-1,:); 
        mu_a(k,:) = mu_a(k-1,:); 
        
        pi_r(k,:) = pi_r(k-1,:);
         pi_a(k,:) = pi_a(k-1,:);

        muhat_r(k,:) = muhat_r(k-1,:);
        muhat_a(k,:) = muhat_a(k-1,:);
        pihat_r(k,:) = pihat_r(k-1,:);
        pihat_a(k,:) = pihat_a(k-1,:);
        
        v_r(k,:)  = v_r(k-1,:);
        v_a(k,:)  = v_a(k-1,:);
        
        w_r(k,:)  = w_r(k-1,:);
        w_a(k,:)  = w_a(k-1,:);
        
        da_r(k,:) = da_r(k-1,:);
        da_a(k,:) = da_a(k-1,:);
        
    end
end
 
% Implied learning rate at the first level
sgmmu_r_2 = tapas_sgm(ka_r(1) *mu_r(:,2), 1);
dasgmmu_r_2 = u_r -sgmmu_r_2;
lr1_r    = diff(sgmmu_r_2)./dasgmmu_r_2(2:n_r,1);
lr1_r(da_r(2:n_r,1)==0) = 0;

sgmmu_a_2 = tapas_sgm(ka_a(1) *mu_a(:,2), 1);
dasgmmu_a_2 = u_a -sgmmu_a_2;
lr1_a    = diff(sgmmu_a_2)./dasgmmu_a_2(2:n_a,1);
lr1_a(da_r(2:n_a,1)==0) = 0;

% Remove representation priors
mu_r(1,:)  = [];
mu_a(1,:)  = [];
pi_r(1,:)  = [];
pi_a(1,:)  = [];


% 
% Check validity of trajectories in reward learning
if any(isnan(mu_r(:))) || any(isnan(pi_r(:)))
    error('tapas:hgf:VarApproxInvalid', 'Variational approximation invalid in reward learning. Parameters are in a region where model assumptions are violated.');
else
    % Check for implausible jumps in trajectories
    dmu_r = diff(mu_r(:,2:end));
    dpi_r = diff(pi_r(:,2:end));
    rmdmu_r = repmat(sqrt(mean(dmu_r.^2)),length(dmu_r),1);
    rmdpi_r = repmat(sqrt(mean(dpi_r.^2)),length(dpi_r),1);

    jumpTol = 16;
    if any(abs(dmu_r(:)) > jumpTol*rmdmu_r(:)) || any(abs(dpi_r(:)) > jumpTol*rmdpi_r(:))
        error('tapas:hgf:VarApproxInvalid', 'Variational approximation invalid in reward learning. Parameters are in a region where model assumptions are violated.');
    end
end

% Check validity of trajectories in social learning
if any(isnan(mu_a(:))) || any(isnan(pi_a(:)))
    error('tapas:hgf:VarApproxInvalid', 'Variational approximation invalid in social learning. Parameters are in a region where model assumptions are violated.');
else
    % Check for implausible jumps in trajectories
    dmu_a = diff(mu_a(:,2:end));
    dpi_a = diff(pi_a(:,2:end));
    rmdmu_a = repmat(sqrt(mean(dmu_a.^2)),length(dmu_a),1);
    rmdpi_a = repmat(sqrt(mean(dpi_a.^2)),length(dpi_a),1);

    jumpTol = 16;
    if any(abs(dmu_a(:)) > jumpTol*rmdmu_a(:)) || any(abs(dpi_a(:)) > jumpTol*rmdpi_a(:))
        error('tapas:hgf:VarApproxInvalid', 'Variational approximation invalid in social learning. Parameters are in a region where model assumptions are violated.');
    end
end

% Remove other dummy initial values
muhat_r(1,:) = [];
muhat_a(1,:) = [];
pihat_r(1,:) = [];
pihat_a(1,:) = [];
v_r(1,:)     = [];
v_a(1,:)     = [];
w_r(1,:)     = [];
w_a(1,:)     = [];
da_r(1,:)    = [];
da_a(1,:)    = [];

% Create result data structure
traj = struct;

traj.mu_r     = mu_r;
traj.mu_a     = mu_a;

traj.sa_r     = 1./pi_r;
traj.sa_a     = 1./pi_a;

traj.muhat_r  = muhat_r;
traj.muhat_a  = muhat_a;

traj.sahat_r  = 1./pihat_r;
traj.sahat_a = 1./pihat_a;

traj.v_r      = v_r;
traj.v_a      = v_a;
traj.w_r      = w_r;
traj.w_a     = w_a;
traj.da_r     = da_r;
traj.da_a     = da_a;

% Updates with respect to prediction
traj.ud_r = mu_r -muhat_r;
traj.ud_a = mu_a -muhat_a;

% Psi (precision weights on prediction errors)
psi_r        = NaN(n_r-1,l);
psi_r(:,2)   = 1./pi_r(:,2);
psi_r(:,3:l) = pihat_r(:,2:l-1)./pi_r(:,3:l);
traj.psi_r   = psi_r;

psi_a        = NaN(n_a-1,l);
psi_a(:,2)   = 1./pi_a(:,2);
psi_a(:,3:l) = pihat_a(:,2:l-1)./pi_a(:,3:l);
traj.psi_a   = psi_a;

% Epsilons (precision-weighted prediction errors)
epsi_r        = NaN(n_r-1,l);
epsi_r(:,2:l) = psi_r(:,2:l) .*da_r(:,1:l-1);
traj.epsi_r   = epsi_r;

epsi_a        = NaN(n_a-1,l);
epsi_a(:,2:l) = psi_a(:,2:l) .*da_a(:,1:l-1);
traj.epsi_a   = epsi_a;


% Full learning rate (full weights on prediction errors)
wt_r        = NaN(n_r-1,l);
wt_r(:,1)   = lr1_r;
wt_r(:,2)   = psi_r(:,2);
wt_r(:,3:l) = 1/2 *(v_r(:,2:l-1) *diag(ka_r(2:l-1))) .*psi_r(:,3:l);
traj.wt_r   = wt_r;

wt_a        = NaN(n_a-1,l);
wt_a(:,1)   = lr1_a;
wt_a(:,2)   = psi_a(:,2);
wt_a(:,3:l) = 1/2 *(v_a(:,2:l-1) *diag(ka_a(2:l-1))) .*psi_a(:,3:l);
traj.wt_a   = wt_a;

% Create matrices for use by the observation model
infStates = NaN(n_r-1,l,9);
infStates(:,:,1) = traj.muhat_r;
infStates(:,:,2) = traj.sahat_r;
infStates(:,:,3) = traj.muhat_a;
infStates(:,:,4) = traj.sahat_a;
infStates(:,:,5) = traj.mu_r;
infStates(:,:,6) = traj.sa_r;
infStates(:,:,7) = traj.mu_a;
infStates(:,:,8) = traj.sa_a;
infStates(:,:,9) = [];

return;
